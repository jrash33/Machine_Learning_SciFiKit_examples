{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jashcrof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jashcrof\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create a computation graph\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y +y +2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#open tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "#initialize the variables\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "\n",
    "#evaluate f\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeating sess.run() is cumbersome so there is a better way\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of manually running the initializer for every single variable use this function\n",
    "init = tf.global_variables_initializer() #prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #actually initializes all the variables\n",
    "    result=f.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "#you can also create an interactive session which automatically sets itself as the default \n",
    "#(don't need a block but need to manually close)\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression w/ TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.6959320e+01],\n",
       "       [ 4.3698898e-01],\n",
       "       [ 9.4245886e-03],\n",
       "       [-1.0791138e-01],\n",
       "       [ 6.4842808e-01],\n",
       "       [-3.9986235e-06],\n",
       "       [-3.7866351e-03],\n",
       "       [-4.2142656e-01],\n",
       "       [-4.3467718e-01]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = .01\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize constants\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "#first just use a random theta for equation: (theta - nMSE(theta))\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred-y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta-learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.428481\n",
      "Epoch 100 MSE = 4.88447\n",
      "Epoch 200 MSE = 4.807394\n",
      "Epoch 300 MSE = 4.8039184\n",
      "Epoch 400 MSE = 4.803636\n",
      "Epoch 500 MSE = 4.803559\n",
      "Epoch 600 MSE = 4.8035054\n",
      "Epoch 700 MSE = 4.8034625\n",
      "Epoch 800 MSE = 4.803427\n",
      "Epoch 900 MSE = 4.803398\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.288693\n",
      "8.109467\n",
      "7.940908\n",
      "7.782323\n",
      "7.633064\n",
      "7.492534\n",
      "7.3601747\n",
      "7.23546\n",
      "7.117909\n",
      "7.007069\n",
      "6.90252\n",
      "6.8038654\n",
      "6.710742\n",
      "6.622807\n",
      "6.539744\n",
      "6.461252\n",
      "6.387055\n",
      "6.3168926\n",
      "6.250522\n",
      "6.1877174\n",
      "6.1282663\n",
      "6.0719695\n",
      "6.018645\n",
      "5.968116\n",
      "5.9202213\n",
      "5.874808\n",
      "5.8317337\n",
      "5.790865\n",
      "5.752079\n",
      "5.715254\n",
      "5.680283\n",
      "5.6470613\n",
      "5.6154923\n",
      "5.585486\n",
      "5.5569534\n",
      "5.529817\n",
      "5.5040007\n",
      "5.4794326\n",
      "5.456047\n",
      "5.4337797\n",
      "5.412571\n",
      "5.3923674\n",
      "5.3731136\n",
      "5.3547616\n",
      "5.3372645\n",
      "5.3205786\n",
      "5.304661\n",
      "5.289473\n",
      "5.2749786\n",
      "5.2611413\n",
      "5.24793\n",
      "5.235311\n",
      "5.2232575\n",
      "5.2117386\n",
      "5.200731\n",
      "5.190208\n",
      "5.1801453\n",
      "5.170522\n",
      "5.1613164\n",
      "5.1525083\n",
      "5.1440797\n",
      "5.13601\n",
      "5.128285\n",
      "5.1208854\n",
      "5.1138\n",
      "5.10701\n",
      "5.100505\n",
      "5.094269\n",
      "5.088291\n",
      "5.082559\n",
      "5.077061\n",
      "5.071787\n",
      "5.0667276\n",
      "5.0618715\n",
      "5.0572104\n",
      "5.052736\n",
      "5.048437\n",
      "5.0443096\n",
      "5.040344\n",
      "5.0365324\n",
      "5.0328703\n",
      "5.0293484\n",
      "5.025964\n",
      "5.022707\n",
      "5.019574\n",
      "5.0165596\n",
      "5.013658\n",
      "5.010866\n",
      "5.008177\n",
      "5.0055866\n",
      "5.0030904\n",
      "5.0006876\n",
      "4.998369\n",
      "4.996135\n",
      "4.993981\n",
      "4.9919033\n",
      "4.989898\n",
      "4.987963\n",
      "4.986096\n",
      "4.984292\n",
      "4.982551\n",
      "4.980868\n",
      "4.9792423\n",
      "4.9776707\n",
      "4.9761505\n",
      "4.9746804\n",
      "4.9732594\n",
      "4.9718823\n",
      "4.9705505\n",
      "4.969261\n",
      "4.968011\n",
      "4.9668\n",
      "4.965627\n",
      "4.964489\n",
      "4.9633856\n",
      "4.962315\n",
      "4.961277\n",
      "4.9602685\n",
      "4.9592886\n",
      "4.9583373\n",
      "4.957413\n",
      "4.9565144\n",
      "4.95564\n",
      "4.9547896\n",
      "4.9539614\n",
      "4.953157\n",
      "4.9523716\n",
      "4.9516087\n",
      "4.950863\n",
      "4.9501376\n",
      "4.94943\n",
      "4.9487395\n",
      "4.9480658\n",
      "4.9474077\n",
      "4.9467654\n",
      "4.946138\n",
      "4.9455247\n",
      "4.944926\n",
      "4.944339\n",
      "4.943764\n",
      "4.9432025\n",
      "4.942652\n",
      "4.942114\n",
      "4.941587\n",
      "4.9410696\n",
      "4.9405622\n",
      "4.940065\n",
      "4.939577\n",
      "4.939098\n",
      "4.938628\n",
      "4.938165\n",
      "4.937712\n",
      "4.9372663\n",
      "4.9368277\n",
      "4.936396\n",
      "4.9359717\n",
      "4.935555\n",
      "4.9351435\n",
      "4.934739\n",
      "4.93434\n",
      "4.9339466\n",
      "4.93356\n",
      "4.933178\n",
      "4.9328012\n",
      "4.9324293\n",
      "4.9320636\n",
      "4.9317017\n",
      "4.931344\n",
      "4.930991\n",
      "4.9306417\n",
      "4.930298\n",
      "4.929957\n",
      "4.9296207\n",
      "4.9292884\n",
      "4.928958\n",
      "4.928633\n",
      "4.9283104\n",
      "4.927991\n",
      "4.927675\n",
      "4.9273624\n",
      "4.9270525\n",
      "4.926745\n",
      "4.9264407\n",
      "4.92614\n",
      "4.925841\n",
      "4.9255433\n",
      "4.9252505\n",
      "4.924958\n",
      "4.9246697\n",
      "4.924383\n",
      "4.924099\n",
      "4.923816\n",
      "4.9235353\n",
      "4.9232574\n",
      "4.9229803\n",
      "4.922706\n",
      "4.922434\n",
      "4.9221625\n",
      "4.921893\n",
      "4.9216266\n",
      "4.92136\n",
      "4.921096\n",
      "4.920834\n",
      "4.9205728\n",
      "4.920313\n",
      "4.920055\n",
      "4.9197993\n",
      "4.9195433\n",
      "4.9192896\n",
      "4.9190373\n",
      "4.918786\n",
      "4.9185357\n",
      "4.9182878\n",
      "4.9180403\n",
      "4.9177947\n",
      "4.9175487\n",
      "4.917305\n",
      "4.9170613\n",
      "4.9168205\n",
      "4.9165797\n",
      "4.9163394\n",
      "4.916101\n",
      "4.9158635\n",
      "4.9156265\n",
      "4.915391\n",
      "4.915156\n",
      "4.9149227\n",
      "4.9146895\n",
      "4.9144573\n",
      "4.9142265\n",
      "4.9139957\n",
      "4.9137673\n",
      "4.913538\n",
      "4.9133105\n",
      "4.913083\n",
      "4.9128575\n",
      "4.9126315\n",
      "4.912407\n",
      "4.912184\n",
      "4.9119596\n",
      "4.9117374\n",
      "4.9115157\n",
      "4.911295\n",
      "4.9110746\n",
      "4.910855\n",
      "4.910636\n",
      "4.910418\n",
      "4.9102\n",
      "4.9099836\n",
      "4.9097676\n",
      "4.909552\n",
      "4.909337\n",
      "4.9091225\n",
      "4.908909\n",
      "4.908696\n",
      "4.908483\n",
      "4.908272\n",
      "4.9080606\n",
      "4.907849\n",
      "4.907639\n",
      "4.907429\n",
      "4.907221\n",
      "4.907013\n",
      "4.906804\n",
      "4.9065976\n",
      "4.90639\n",
      "4.9061837\n",
      "4.9059777\n",
      "4.905772\n",
      "4.9055686\n",
      "4.9053645\n",
      "4.9051614\n",
      "4.904958\n",
      "4.904755\n",
      "4.904553\n",
      "4.904351\n",
      "4.90415\n",
      "4.9039497\n",
      "4.9037495\n",
      "4.9035506\n",
      "4.9033513\n",
      "4.9031515\n",
      "4.902954\n",
      "4.902757\n",
      "4.9025593\n",
      "4.902363\n",
      "4.9021664\n",
      "4.9019704\n",
      "4.901775\n",
      "4.90158\n",
      "4.9013853\n",
      "4.9011917\n",
      "4.900998\n",
      "4.900806\n",
      "4.9006133\n",
      "4.90042\n",
      "4.9002285\n",
      "4.900038\n",
      "4.899847\n",
      "4.8996553\n",
      "4.8994665\n",
      "4.8992763\n",
      "4.899087\n",
      "4.898898\n",
      "4.8987103\n",
      "4.8985224\n",
      "4.8983355\n",
      "4.8981476\n",
      "4.8979616\n",
      "4.8977737\n",
      "4.8975887\n",
      "4.8974032\n",
      "4.897218\n",
      "4.897034\n",
      "4.896849\n",
      "4.896665\n",
      "4.896482\n",
      "4.8963\n",
      "4.8961163\n",
      "4.8959346\n",
      "4.8957524\n",
      "4.8955708\n",
      "4.8953896\n",
      "4.895209\n",
      "4.8950286\n",
      "4.8948493\n",
      "4.89467\n",
      "4.8944907\n",
      "4.894312\n",
      "4.894134\n",
      "4.8939557\n",
      "4.893778\n",
      "4.893601\n",
      "4.8934236\n",
      "4.8932476\n",
      "4.8930717\n",
      "4.8928957\n",
      "4.8927197\n",
      "4.892545\n",
      "4.8923707\n",
      "4.8921967\n",
      "4.8920226\n",
      "4.8918495\n",
      "4.891676\n",
      "4.8915033\n",
      "4.8913307\n",
      "4.891158\n",
      "4.890987\n",
      "4.8908157\n",
      "4.8906446\n",
      "4.890474\n",
      "4.890304\n",
      "4.890134\n",
      "4.889964\n",
      "4.8897953\n",
      "4.889626\n",
      "4.889458\n",
      "4.88929\n",
      "4.8891225\n",
      "4.888953\n",
      "4.888787\n",
      "4.8886204\n",
      "4.888454\n",
      "4.8882885\n",
      "4.888122\n",
      "4.887957\n",
      "4.887791\n",
      "4.8876276\n",
      "4.8874626\n",
      "4.887298\n",
      "4.887135\n",
      "4.8869715\n",
      "4.886808\n",
      "4.886646\n",
      "4.886483\n",
      "4.886322\n",
      "4.88616\n",
      "4.8859982\n",
      "4.885837\n",
      "4.8856764\n",
      "4.8855166\n",
      "4.8853564\n",
      "4.885197\n",
      "4.885038\n",
      "4.884878\n",
      "4.8847194\n",
      "4.884561\n",
      "4.884403\n",
      "4.8842463\n",
      "4.8840876\n",
      "4.883931\n",
      "4.8837743\n",
      "4.8836174\n",
      "4.883462\n",
      "4.8833055\n",
      "4.883149\n",
      "4.8829956\n",
      "4.88284\n",
      "4.8826857\n",
      "4.8825307\n",
      "4.882377\n",
      "4.882224\n",
      "4.882071\n",
      "4.8819175\n",
      "4.8817644\n",
      "4.8816123\n",
      "4.88146\n",
      "4.8813086\n",
      "4.8811574\n",
      "4.8810053\n",
      "4.8808546\n",
      "4.880705\n",
      "4.8805537\n",
      "4.8804045\n",
      "4.8802547\n",
      "4.880106\n",
      "4.8799562\n",
      "4.879808\n",
      "4.879659\n",
      "4.879511\n",
      "4.879364\n",
      "4.8792157\n",
      "4.879069\n",
      "4.878922\n",
      "4.878775\n",
      "4.878629\n",
      "4.878484\n",
      "4.878337\n",
      "4.878192\n",
      "4.878046\n",
      "4.877902\n",
      "4.877757\n",
      "4.877613\n",
      "4.8774695\n",
      "4.8773255\n",
      "4.8771815\n",
      "4.8770385\n",
      "4.8768964\n",
      "4.8767533\n",
      "4.8766117\n",
      "4.8764696\n",
      "4.876328\n",
      "4.876187\n",
      "4.876045\n",
      "4.875905\n",
      "4.875764\n",
      "4.8756247\n",
      "4.8754835\n",
      "4.8753443\n",
      "4.875205\n",
      "4.875066\n",
      "4.874926\n",
      "4.8747883\n",
      "4.8746505\n",
      "4.8745127\n",
      "4.874374\n",
      "4.874237\n",
      "4.8740997\n",
      "4.873962\n",
      "4.8738275\n",
      "4.87369\n",
      "4.873554\n",
      "4.873418\n",
      "4.873283\n",
      "4.8731475\n",
      "4.8730125\n",
      "4.8728776\n",
      "4.872743\n",
      "4.872609\n",
      "4.872475\n",
      "4.8723416\n",
      "4.8722086\n",
      "4.872075\n",
      "4.8719425\n",
      "4.87181\n",
      "4.8716774\n",
      "4.8715453\n",
      "4.871414\n",
      "4.871282\n",
      "4.8711514\n",
      "4.8710194\n",
      "4.87089\n",
      "4.8707585\n",
      "4.8706284\n",
      "4.8704987\n",
      "4.8703694\n",
      "4.87024\n",
      "4.8701105\n",
      "4.869982\n",
      "4.869853\n",
      "4.8697243\n",
      "4.869597\n",
      "4.8694687\n",
      "4.869342\n",
      "4.869213\n",
      "4.8690867\n",
      "4.86896\n",
      "4.868833\n",
      "4.8687067\n",
      "4.8685813\n",
      "4.8684554\n",
      "4.8683295\n",
      "4.8682036\n",
      "4.868079\n",
      "4.867954\n",
      "4.8678303\n",
      "4.8677053\n",
      "4.8675814\n",
      "4.867458\n",
      "4.867334\n",
      "4.867211\n",
      "4.8670874\n",
      "4.866965\n",
      "4.8668423\n",
      "4.8667192\n",
      "4.8665977\n",
      "4.866476\n",
      "4.866354\n",
      "4.866233\n",
      "4.866112\n",
      "4.8659906\n",
      "4.8658705\n",
      "4.8657503\n",
      "4.8656297\n",
      "4.8655095\n",
      "4.865391\n",
      "4.8652706\n",
      "4.865151\n",
      "4.865032\n",
      "4.8649135\n",
      "4.864795\n",
      "4.8646774\n",
      "4.8645587\n",
      "4.8644423\n",
      "4.864323\n",
      "4.8642063\n",
      "4.864089\n",
      "4.863972\n",
      "4.863856\n",
      "4.863739\n",
      "4.8636236\n",
      "4.863507\n",
      "4.863392\n",
      "4.863276\n",
      "4.863161\n",
      "4.863045\n",
      "4.862931\n",
      "4.862816\n",
      "4.862702\n",
      "4.862588\n",
      "4.862474\n",
      "4.86236\n",
      "4.862247\n",
      "4.8621335\n",
      "4.862021\n",
      "4.8619084\n",
      "4.861795\n",
      "4.8616824\n",
      "4.8615713\n",
      "4.861459\n",
      "4.861347\n",
      "4.8612356\n",
      "4.861125\n",
      "4.8610134\n",
      "4.8609033\n",
      "4.860793\n",
      "4.8606825\n",
      "4.860572\n",
      "4.8604627\n",
      "4.8603525\n",
      "4.8602433\n",
      "4.860134\n",
      "4.860025\n",
      "4.8599157\n",
      "4.859808\n",
      "4.859699\n",
      "4.8595905\n",
      "4.8594837\n",
      "4.859376\n",
      "4.8592677\n",
      "4.859161\n",
      "4.8590536\n",
      "4.8589473\n",
      "4.85884\n",
      "4.858733\n",
      "4.858627\n",
      "4.858522\n",
      "4.8584156\n",
      "4.8583097\n",
      "4.858205\n",
      "4.8581\n",
      "4.8579946\n",
      "4.85789\n",
      "4.8577843\n",
      "4.8576813\n",
      "4.8575764\n",
      "4.8574724\n",
      "4.857369\n",
      "4.8572655\n",
      "4.857162\n",
      "4.8570585\n",
      "4.8569555\n",
      "4.8568535\n",
      "4.856751\n",
      "4.856648\n",
      "4.856546\n",
      "4.8564444\n",
      "4.8563433\n",
      "4.8562417\n",
      "4.8561397\n",
      "4.8560386\n",
      "4.855938\n",
      "4.8558373\n",
      "4.8557363\n",
      "4.8556366\n",
      "4.855536\n",
      "4.8554363\n",
      "4.8553367\n",
      "4.855237\n",
      "4.855138\n",
      "4.8550386\n",
      "4.85494\n",
      "4.8548408\n",
      "4.8547425\n",
      "4.8546443\n",
      "4.854546\n",
      "4.8544483\n",
      "4.8543506\n",
      "4.854253\n",
      "4.854155\n",
      "4.8540583\n",
      "4.8539615\n",
      "4.8538637\n",
      "4.853768\n",
      "4.8536706\n",
      "4.8535748\n",
      "4.85348\n",
      "4.8533835\n",
      "4.8532877\n",
      "4.853192\n",
      "4.8530965\n",
      "4.853002\n",
      "4.8529067\n",
      "4.852812\n",
      "4.852717\n",
      "4.852623\n",
      "4.852529\n",
      "4.8524346\n",
      "4.8523407\n",
      "4.8522477\n",
      "4.8521547\n",
      "4.8520613\n",
      "4.8519683\n",
      "4.8518744\n",
      "4.851782\n",
      "4.85169\n",
      "4.8515973\n",
      "4.851505\n",
      "4.8514132\n",
      "4.8513217\n",
      "4.85123\n",
      "4.851139\n",
      "4.8510475\n",
      "4.850956\n",
      "4.850865\n",
      "4.8507743\n",
      "4.8506837\n",
      "4.850593\n",
      "4.850503\n",
      "4.850414\n",
      "4.8503227\n",
      "4.8502345\n",
      "4.8501444\n",
      "4.850055\n",
      "4.849966\n",
      "4.8498774\n",
      "4.8497887\n",
      "4.8496995\n",
      "4.849611\n",
      "4.8495235\n",
      "4.849435\n",
      "4.8493476\n",
      "4.8492594\n",
      "4.8491716\n",
      "4.849084\n",
      "4.848997\n",
      "4.8489103\n",
      "4.848823\n",
      "4.848737\n",
      "4.8486505\n",
      "4.848563\n",
      "4.8484774\n",
      "4.848391\n",
      "4.848305\n",
      "4.84822\n",
      "4.848134\n",
      "4.848049\n",
      "4.847964\n",
      "4.8478785\n",
      "4.847794\n",
      "4.8477097\n",
      "4.8476243\n",
      "4.84754\n",
      "4.847456\n",
      "4.8473725\n",
      "4.847288\n",
      "4.847204\n",
      "4.8471203\n",
      "4.847038\n",
      "4.8469543\n",
      "4.8468714\n",
      "4.846789\n",
      "4.846705\n",
      "4.846623\n",
      "4.8465405\n",
      "4.8464584\n",
      "4.8463764\n",
      "4.8462944\n",
      "4.846212\n",
      "4.8461313\n",
      "4.84605\n",
      "4.8459682\n",
      "4.845887\n",
      "4.8458066\n",
      "4.845725\n",
      "4.845645\n",
      "4.8455644\n",
      "4.845484\n",
      "4.845404\n",
      "4.845324\n",
      "4.845244\n",
      "4.8451643\n",
      "4.8450847\n",
      "4.845006\n",
      "4.844926\n",
      "4.8448467\n",
      "4.844768\n",
      "4.8446894\n",
      "4.8446107\n",
      "4.8445325\n",
      "4.8444533\n",
      "4.8443756\n",
      "4.8442974\n",
      "4.8442197\n",
      "4.844142\n",
      "4.8440647\n",
      "4.843988\n",
      "4.8439107\n",
      "4.8438334\n",
      "4.8437552\n",
      "4.8436804\n",
      "4.843602\n",
      "4.843526\n",
      "4.8434496\n",
      "4.8433733\n",
      "4.843298\n",
      "4.8432217\n",
      "4.8431454\n",
      "4.84307\n",
      "4.8429947\n",
      "4.842919\n",
      "4.842844\n",
      "4.8427687\n",
      "4.8426933\n",
      "4.8426194\n",
      "4.842545\n",
      "4.8424706\n",
      "4.8423963\n",
      "4.842323\n",
      "4.842248\n",
      "4.842174\n",
      "4.8421\n",
      "4.8420267\n",
      "4.8419538\n",
      "4.84188\n",
      "4.8418074\n",
      "4.841734\n",
      "4.8416605\n",
      "4.841588\n",
      "4.841515\n",
      "4.8414435\n",
      "4.8413706\n",
      "4.8412986\n",
      "4.841227\n",
      "4.8411555\n",
      "4.8410845\n",
      "4.841012\n",
      "4.84094\n",
      "4.8408685\n",
      "4.8407974\n",
      "4.840727\n",
      "4.840656\n",
      "4.840585\n",
      "4.8405147\n",
      "4.840444\n",
      "4.8403735\n",
      "4.840303\n",
      "4.840233\n",
      "4.840163\n",
      "4.840094\n",
      "4.840024\n",
      "4.8399544\n",
      "4.8398848\n",
      "4.839815\n",
      "4.839747\n",
      "4.8396773\n",
      "4.839608\n",
      "4.83954\n",
      "4.8394713\n",
      "4.839402\n",
      "4.8393345\n",
      "4.8392663\n",
      "4.8391986\n",
      "4.83913\n",
      "4.8390627\n",
      "4.8389945\n",
      "4.8389277\n",
      "4.8388596\n",
      "4.838793\n",
      "4.838726\n",
      "4.838659\n",
      "4.8385916\n",
      "4.838525\n",
      "4.8384576\n",
      "4.838392\n",
      "4.838325\n",
      "4.8382583\n",
      "4.8381925\n",
      "4.8381267\n",
      "4.8380604\n",
      "4.837995\n",
      "4.8379292\n",
      "4.837864\n",
      "4.837799\n",
      "4.8377337\n",
      "4.837669\n",
      "4.837603\n",
      "4.8375387\n",
      "4.8374734\n",
      "4.8374095\n",
      "4.8373446\n",
      "4.8372808\n",
      "4.837216\n",
      "4.837152\n",
      "4.837088\n",
      "4.8370247\n",
      "4.836961\n",
      "4.836897\n",
      "4.8368335\n",
      "4.83677\n",
      "4.8367076\n",
      "4.836643\n",
      "4.8365808\n",
      "4.8365183\n",
      "4.8364553\n",
      "4.8363934\n",
      "4.83633\n",
      "4.8362675\n",
      "4.836205\n",
      "4.836143\n",
      "4.8360815\n",
      "4.8360195\n",
      "4.835958\n",
      "4.835896\n",
      "4.8358335\n",
      "4.8357725\n",
      "4.835711\n",
      "4.8356504\n",
      "4.835589\n",
      "4.835528\n",
      "4.8354683\n",
      "4.835407\n",
      "4.835346\n",
      "4.8352857\n",
      "4.835225\n",
      "4.8351655\n",
      "4.835105\n",
      "4.8350444\n",
      "4.8349853\n",
      "4.834925\n",
      "4.8348665\n",
      "4.834806\n",
      "4.8347464\n",
      "4.8346877\n",
      "4.8346276\n",
      "4.834569\n",
      "4.8345103\n",
      "4.834451\n",
      "4.8343925\n",
      "4.8343325\n",
      "4.8342752\n",
      "4.8342166\n",
      "4.8341584\n",
      "4.8341002\n",
      "4.8340416\n",
      "4.833984\n",
      "4.833926\n",
      "4.833868\n",
      "4.833811\n",
      "4.833753\n",
      "4.8336954\n",
      "4.8336387\n",
      "4.833581\n",
      "4.833524\n",
      "4.8334675\n",
      "4.8334103\n",
      "4.833354\n",
      "4.833297\n",
      "4.833241\n",
      "4.8331842\n",
      "4.833128\n",
      "4.833071\n",
      "4.8330154\n",
      "4.832959\n",
      "4.8329034\n",
      "4.832848\n",
      "4.8327923\n",
      "4.832737\n",
      "4.8326807\n",
      "4.832626\n",
      "4.8325706\n",
      "4.8325157\n",
      "4.8324604\n",
      "4.8324056\n",
      "4.8323503\n",
      "4.8322964\n",
      "4.8322415\n",
      "4.832186\n",
      "4.832133\n",
      "4.8320775\n",
      "4.8320236\n",
      "4.8319707\n",
      "4.8319154\n",
      "4.8318615\n",
      "4.8318086\n",
      "4.831754\n",
      "4.8317003\n",
      "4.8316474\n",
      "4.831594\n",
      "4.831541\n",
      "4.8314877\n",
      "4.8314347\n",
      "4.831382\n",
      "4.831329\n",
      "4.8312755\n",
      "4.8312235\n",
      "4.831171\n",
      "4.831119\n",
      "4.831066\n",
      "4.8310137\n",
      "4.8309617\n",
      "4.83091\n",
      "4.8308578\n",
      "4.830806\n",
      "4.8307543\n",
      "4.8307023\n",
      "4.8306513\n",
      "4.8306\n",
      "4.8305473\n",
      "4.830497\n",
      "4.8304462\n",
      "4.8303947\n",
      "4.8303432\n",
      "4.830292\n",
      "4.8302426\n",
      "4.830191\n",
      "4.830141\n",
      "4.8300896\n",
      "4.8300395\n",
      "4.8299894\n",
      "4.8299394\n",
      "4.82989\n",
      "4.8298397\n",
      "4.829789\n",
      "4.8297396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.82969\n",
      "4.8296404\n",
      "4.829591\n",
      "4.829541\n",
      "4.829492\n",
      "4.829443\n",
      "4.829394\n",
      "4.8293447\n",
      "4.829296\n",
      "4.829247\n",
      "4.829198\n",
      "4.8291492\n",
      "4.8291006\n",
      "4.8290524\n",
      "4.829004\n",
      "4.8289557\n",
      "4.8289075\n",
      "4.8288593\n",
      "4.8288107\n",
      "4.8287635\n",
      "4.8287153\n",
      "4.828668\n",
      "4.8286195\n",
      "4.8285723\n",
      "4.828525\n"
     ]
    }
   ],
   "source": [
    "#closer look\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(0,1000):\n",
    "        print(mse.eval())\n",
    "        sess.run(training_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff/optimizer to automatically calculate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous requires you to mathematically derive gradients from the cost function MSE\n",
    "#linear regression isn't too bad but it is a headache with more advanced algorithms\n",
    "#initialize constants\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred-y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#auto************************************\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "training_op = tf.assign(theta, theta-learning_rate*gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.918679\n",
      "Epoch 100 MSE = 5.0377765\n",
      "Epoch 200 MSE = 4.93862\n",
      "Epoch 300 MSE = 4.9015946\n",
      "Epoch 400 MSE = 4.875283\n",
      "Epoch 500 MSE = 4.8561325\n",
      "Epoch 600 MSE = 4.8421664\n",
      "Epoch 700 MSE = 4.831963\n",
      "Epoch 800 MSE = 4.8244977\n",
      "Epoch 900 MSE = 4.8190227\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred-y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "#auto************************************\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#could also use momentum optimizer\n",
    "#optimizer2 = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=.9)\n",
    "\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 6.415772\n",
      "Epoch 100 MSE = 4.989398\n",
      "Epoch 200 MSE = 4.92045\n",
      "Epoch 300 MSE = 4.8883715\n",
      "Epoch 400 MSE = 4.865686\n",
      "Epoch 500 MSE = 4.8491817\n",
      "Epoch 600 MSE = 4.8371296\n",
      "Epoch 700 MSE = 4.8283124\n",
      "Epoch 800 MSE = 4.821848\n",
      "Epoch 900 MSE = 4.817099\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "#use placeholder nodes to replace X/y at every iteration\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1,2,3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6], [7,8,9]]})\n",
    "    \n",
    "    print(B_val_1)\n",
    "    print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make X,y placeholder nodes\n",
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the batch size and compute the total number of batches\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the mini batches one by one, then provide the value of X and y via feed_dict when evaluating a node\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    [...] #load data from disk\n",
    "    return X_batch, y_batch\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
